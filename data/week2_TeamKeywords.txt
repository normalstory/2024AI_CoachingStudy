<K-최근접 이웃 알고리즘> (KNN, K-Nearest Neighbors Algorithm)
지도학습(supervised learning)에서 사용되는 분류(classification) 및 회귀(regression) 알고리즘 중 하나입니다. KNN 알고리즘은 새로운 데이터가 주어졌을 때, 이 데이터와 가장 가까운 K개의 이웃 데이터들을 찾아서 이 데이터의 클래스 또는 값을 예측하는 방식으로 동작합니다. 여기서 이웃 데이터들은 거리(distance)를 기반으로 판단됩니다. 즉, 가장 가까운 거리에 있는 K개의 데이터들을 선택하여 그들이 속한 클래스나 값으로 예측하는 것입니다.
(출처 - 위키, K-최근접 이웃(K-NN) 알고리즘 )


<K-평균 알고리즘> (K-mean Algorithm)
주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 
동작한다. k-평균 클러스터링 알고리즘은 클러스터링 방법 중 분할법에 속한다. 분할법은 주어진 데이터를 여러 파티션 (그룹)으로 나누는 방법이다.
K-means 알고리즘은 K-NN 알고리즘과 유사하면서도 다르다. 둘은 모두 K개의 점을 지정하여 거리를 기반으로 구현되는 거리기반 분석 알고리즘이다. 
하지만 둘의 차이점은 지도학습에 속하는 K-NN 알고리즘과 달리 K-means 알고리즘은 비지도학습 방법에 속한다. 따라서 K-means 알고리즘의 목적을 분류(Classification)라고 하지 않고 군집화(Clustering)라고 한다. 즉, K-NN은 분류 알고리즘, K-means은 군집화 알고리즘이라고 할 수 있다. 
(출처 - link, 위키; 이미지: 생활코딩)


<정규화, 정칙화, 규제화, 규칙화> (Regularization)
기계학습에서 과적합(Overfitting)을 방지하기 위해 사용되는 기법이다. 과적합은 모델이 학습 데이터에 지나치게 잘 맞춰져 있어 새로운 데이터에 대한 일반화 능력이 떨어지는 현상을 말한다. 정규화는 모델의 복잡도를 제한하여 이러한 문제를 완화한다. 
regularization의 의미는 학습 데이터에만 너무 특화되지 않고 ‘일반화(generalization)’가 가능하도록 일종의 규제(penalty)를 가하는 기법을 말한다.
가장 일반적인 정규화 기법으로는 L1 정규화(Lasso)와 L2 정규화(Ridge)가 있다. L1 정규화는 모델의 가중치에 대한 절대값의 합을 손실 함수에 추가하며, 이는 모델의 일부 가중치를 정확히 0으로 만들어 특성 선택의 효과를 낸다. 반면, L2 정규화는 가중치의 제곱의 합을 손실 함수에 추가하여, 모든 가중치가 0에 가까워지도록 만들지만 정확히 0이 되지는 않는다. 이러한 정규화 기법들은 모델이 데이터의 중요한 패턴을 학습하면서도 과도하게 복잡해지는 것을 방지한다. 
말 그대로 '제약'을 거는 작업. ​여기서 제약을 건다는 것은 모델을 좀 더 complex 혹은 flexible 하게 만든다
(출처 - link, regularization (정규화, 정칙화, 규제화, 규칙화) - 인공지능(AI) & 머신러닝(ML) 사전)


<드롭아웃> (dropout)
신경망 학습 과정에서 과적합을 방지하기 위해 사용되는 기법이다. 이 방법은 학습 단계마다 무작위로 일부 뉴런을 비활성화(즉, 드롭아웃) 시킴으로써, 신경망이 특정 뉴런에 과도하게 의존하는 것을 방지한다. 드롭아웃은 각 학습 단계에서 랜덤하게 선택된 뉴런을 일시적으로 제거함으로써, 네트워크가 더 강건하게 데이터의 다양한 패턴을 학습하도록 돕는다. 
드롭아웃은 일반적으로 완전 연결 계층(Fully Connected Layers)에서 주로 사용되지만, 컨볼루션 신경망(CNN)과 같은 다른 유형의 신경망에서도 적용될 수 있다. 드롭아웃 비율, 즉 각 학습 단계에서 비활성화할 뉴런의 비율은 하이퍼파라미터로 설정되며, 이 값에 따라 모델의 학습과 일반화 능력이 달라질 수 있다.
작동방식은 레이어의 사이마다 드롭아웃을 실행하면 훈련이 반복되는 과정 중에 무작위로 해당 레이어의 뉴런을 삭제하여 이를 가능하게 한다.드롭아웃을 적용하면 신경망의 과적합을 방지하는 데 도움을 준다고 알려져 있습니다. 하지만 드롭아웃 층과 배치 정규화 층은 모두 정규화 기능을 포함하고 있으므로 두 가지 층을 같이 사용한다면 오히려 학습 성능이 좋지 않을 수 있습니다.
(출처 - link)


<정규화> (Normalization)
정규화&표준화가 필요한 이유는 기준을 설정하고 그 기준내에서 각 데이터들을 평가하기 때문에 비교가 편하다. 머신러닝을 하다보면 기본적으로 많은 양의 데이터를 처리하게 된다. 그리고 그 데이터안에 target과 관련있는 특성(feature)을 뽑아낸다. 그런데 특성의 단위도 다르고 그 범위도 다르다면 각 데이터들을 직접적으로 비교할 수가 없게 된다.그래서 각 특성들의 단위를 무시하고 값으로 단순 비교할 수 있게 만들어 줄 필요가 있다. 그것이 우리가 정규화/표준화를 진행하는 이유다. 정규화/표준화가 해주는 것을 특성 스케일링(feature scaling) 또는 데이터 스케일링(data scaling)이라고 한다. 또한 가장 중요한 이유는 scale의 범위가 너무 크면 노이즈 데이터가 생성되거나 overfitting이 될 가능성이 높아지기 때문이다.
(출처 - [ML]정규화(normalization)와 표준화(standardization)는 왜 하는걸까?)


<표준화> (Standardization)  
Normalization과 Standardization 모두 데이터를 좀 더 compact 하게 만들기 위하여 사용합니다. ​즉, 값의 범위(scale)를 축소하는 re-scaling 과정을 의미합니다. scale의 범위가 너무 크면 노이즈 데이터가 생성되거나 overfitting이 될 가능성이 높아집니다. 제곱 손실 함수와 절댓값 손실 함수의 특성을 비교해보아도 그 이유를 짐작할 수 있습니다. 제곱 손실 함수를 사용할 때는 값이 비정상적으로 커져 노이즈 데이터가 많이 생성될 수 있기 때문에 우리는 노이즈 데이터가 너무 많이 생성될 거 같으면 절댓값 손실 함수를 사용하게 됩니다. 또한, ​값이 너무 커지게 되므로 활성화 함수를 거친다고 하여도 한쪽으로 값이 쏠릴 가능성이 높기 때문활성화 함수(activation function)를 거치는 의미가 사라집니다.
(출처 - 머신 러닝 - Normalization, Standardization, Regularization 비교 : 네이버 블로그)
<활성화 함수> (activation function)
활성화 함수는 인공 신경망에서 입력을 변환하는 함수이다. 시그모이드 함수, 쌍곡탄젠트 함수, ReLU 등이 대표적인 활성화 함수이다. 인공 신경망에서 노드의 활성화 함수는 노드의 개별 입력과 가중치를 기반으로 노드의 출력을 계산하는 함수이다.
퍼셉트론(Perceptron)의 출력값을 결정하는 비선형(non-linear) 함수입니다. 즉, 활성화 함수는 퍼셉트론에서 입력값의 총합을 출력할지 말지 결정하고, 출력한다면 어떤 값으로 변환하여 출력할지 결정하는 함수입니다.
(출처 - link, 위키)


<렐루> (ReLU, Rectified Linear Unit)
ReLU 함수는 정류 선형 유닛에 대한 함수이다. ReLU는 입력값이 0보다 작으면 0으로 출력, 0보다 크면 입력값 그대로 출력하는 유닛입니다. 
기존 시그모이드sigmoid와 같은 비선형 활성 함수non-linear activation function들이 갖고 있던 그래디언트 소실 문제라는 단점을 보완하기 위해 제안된 새로운 활성 함수입니다. 
y = ReLU(x) = max(0,x)
음수 구간에서는 전부 0의 값을 지니게 되며, 양수 구간에서는 전부 기울기가 1인 형태가 됩니다. 양수 구간에서의 기울기가 1인 덕분에, 렐루를 통해 구현된 심층신경망은 매우 빠르게 최적화가 가능합니다.
렐루로 인해 기존에 학습할 수 없었던 좀 더 깊은 신경망도 큰 무리 없이 학습시킬 수 있게 되었습니다. 또한 학습이 진행됨에 따라 점점 속도가 더뎌지는 기존의 활성 함수들과 달리, 렐루의 경우에는 손실 값이 떨어지는 속도가 매우 빠릅니다. 렐루는 위 그림과 같이 음수 구간에서는 기울기가 0이 되므로 렐루의 입력이 0이 되는 상황에서는 렐루 뒷 단의 가중치 파라미터들에 대해 학습을 진행할 수 없습니다. 
(출처 - 위키, 렐루(ReLU) )


<리키렐루> (leaky ReLU)
리키렐루는 아래의 수식과 같이 구현되며, 음수 구간에서 비록 1보다 작지만 0이 아닌 기울기를 갖는 것이 특징입니다. 참고로 음수 영역 기울기가 0이 될 경우 일반 렐루가 되고, 1이 될 경우 비선형성을 잃게 됩니다.
y = LeakyReLUα(x) = max(α⋅x,x),  where 0 ≤ α < 1
하지만 렐루와 리키렐루를 사용하더라도 음수 구간은 기울기가 여전히 0보다 작기 때문에 사실 그래디언트 소실 문제가 완전히 해결된 것은 아닙니다. 참고로 이후에 나오게 된 레즈넷ResNet에 활용된 스킵-커넥션skip-connection을 통해 그래디언트 소실을 완벽하게 해결할 수 있게 되었습니다.
(출처 - 렐루(ReLU) )


<유클리드 공간> (Euclidean space)
유클리드가 연구했던 평면과 공간을 일반화한 것이다. 이 일반화는 유클리드가 생각했던 거리와 길이와 각도를 좌표계를 도입하여, 임의 차원의 공간으로 확장한 것이다. 이는 표준적인 유한 차원, 실수, 내적 공간이다.
경우에 따라서는 민코프스키 공간에 대비되는 말로서, 피타고라스의 정리에 의한 길이소의 제곱의 계수가 모두 양수인 공간을 이야기한다.
유클리드 공간은 중력장이 거의 작용하지 않는 공간에서만 실제 세계와 잘 들어맞는 근사적인 이론이다.
(출처 - 위키백과, 유클리드 공간과 기하학 (Euclidean space & geometry))


<역행렬> (inverse matrix)
선형대수학에서 가역 행렬(invertible matrix) 또는 정칙 행렬(regular matrix) 또는 비특이 행렬(non-singular matrix)은 그와 곱한 결과가 단위 행렬인 행렬을 갖는 행렬이다. 이를 그 행렬의 역행렬(inverse matrix)이라고 한다.
, 어떤 행렬 A의 좌, 우측에 곱하여 단위행렬을 만들어주는 행렬을 말한다. 또한, 역행렬은 이러한 이유로 '교환법칙'이 성립한다. 모든 경우에 역행렬이 존재하지는 않는데, 이를 검사하기위해 사용하는 것이 '행렬식'이다. 행렬식(Determinant, 이하 D)의 결과가 '0'이 아닌 경우, 해당 행렬의 역행렬이 존재한다.
(출처 - 위키백과, 선형대수 4강 역행렬과 전치행렬)


<벡터> (vector)
크기와 방향을 가지고 있는 양을 나타내는 개념입니다. 이는 화살표로 표현될 수 있습니다.
, 수학과 물리학에서 벡터는 단일 숫자로 표현할 수 없는 일부 수량이나 일부 벡터 공간의 요소를 비공식적으로 나타내는 용어입니다. 역사적으로 벡터는 변위, 힘, 속도와 같이 크기와 방향을 모두 갖는 수량에 대해 기하학 및 물리학에 도입되었습니다. 
(출처 - 위키독스, 위키)


<스칼라> (scalar)
나의 수치(數値)만으로 완전히 표시되는 양을 의미합니다. 방향의 구별이 없는 물리적 수량입니다. 질량·에너지·밀도(密度)·전기량(電氣量) 등. 간단히 얘기하면 벡터는 크기 + 방향, 스칼라는 크기만 있습니다. 
- 물리 : 스칼라는 하나의 숫자로만 표시되는 양 즉, 단지 크기만 있는 물리량입니다. 벡터, 텐서 등이 방향과 크기가 있는 물리량인데 대하여 방향의 구별이 없는 수량입니다. 
- 수학 : 스칼라(scalar)는 선형대수학에서 선형공간을 정의 할 때, 선형공간의 원소와 스칼라 곱을 하는 체의 원소입니다. 스칼라의 정의는 N차원 공간에서 N의 0승개의 수로 표현할 수 있는 물리량이므로 좌표계가 변환되어도 그에 따라 변화하지 않는 양이라는 것을 알 수 있습니다.
(출처 - 위키독스, 위키-물리, 위키-수학 )


<노름> (Norm)
벡터의 크기 또는 길이를 측정하는 방법이며, 벡터 공간을 어떤 양의 실수 값으로 매핑(mapping)하는 함수와 유사합니다.
(출처 - [선형대수] 5. 놈(노름, norm)과 놈 공간(normed space) : 네이버 블로그)


<스파스 데이터> (sparse data)
스파스 데이터는 대부분의 값이 0인 데이터입니다. 즉, 데이터 세트에서 많은 요소가 실제로는 정보를 담고 있지 않은 상태입니다. 스파스 데이터의 예로는 큰 행렬에서 대부분의 값이 0인 경우를 들 수 있습니다​.
(출처 - SciPy Sparse Data)


<판다스> (Pandas)
Pandas는 쉽고 직관적인 관계형 또는 분류된 데이터로 작업 할 수 있도록 설계된 빠르고 유연하며 표현이 풍부한 데이터 구조를 제공하는 Python 패키지이다. Python에서 실용적인 실제 데이터 분석을 수행하기 위한 고수준의 객체 형태를 목표로한다. 또한, 어떤 언어로도 사용할 수 있는 가장 강력하고 유연한 오픈 소스 데이터 분석 / 조직 도구가되는 더 넓은 목표를 가지고 있다. 
Pandas의 두 가지 주요 데이터 구조인 Series (1차원) 및 DataFrame (2차원)은 재무, 통계, 사회 과학 및 다양한 엔지니어링 분야의 일반적인 사용 사례의 대부분을 처리한다.
( 출처: 판다스(Pandas)란? - 이론, 기초)


<시리즈> (Series)
시리즈(Series)는 DataFrame 중 하나의 Column에 해당하는 데이터 모음 object이다.
, 판다스의 시리즈는 일차원 데이터를 관리하는 자료구조로, 데이터와 함께 인덱스를 사용하여 데이터에 레이블을 달아 둘 수 있다. 리스트와 튜플의 장점을 섞어 놓은 것과 같이 동작한다.
- 파이썬 리스트를 활용하여 시리즈 객체를 생성할 수 있다. 따로 인덱스를 지정해주지 않으면 자동으로 정수 인덱스가 생성된다.
- 파이썬의 딕셔너리와 비슷하게 시리즈도 각 데이터에 인덱스를 설정할 수 있다.
( 출처: 머신러닝을 위한 파이썬 - 파이썬으로 데이터 처리하기, [Pandas] 시리즈(Series) 기본 문법 정리  )


<데이터 프레임> (DataFrame)
데이터프레임(Data Frame)은 Data Table 전체를 포함하는 Object이다.
, 판다스에서 사용되는 가장 중요한 데이터 구조 중 하나예요. 우리가 공부할 때 노트에 정보를 정리하고, 각 정보를 표의 칸에 적어놓는 것처럼 데이터프레임은 컴퓨터로 정보를 정리하고 저장하는 도구예요. 데이터프레임은 행(가로줄)과 열(세로줄)로 이루어져 있어서, 이것을 사용하면 정보를 쉽게 정리하고 찾을 수 있어요.
( 출처: 머신러닝을 위한 파이썬 - 파이썬으로 데이터 처리하기, 인 투 더 파이어썬_ 3) 데이터프레임이란?  )


<위치 인자> (Positional arguments) 
Positional Argument 는 함수를 호출할 때 값을 넘겨주는 방법으로 위치로 그 값을 주고 받는다.
알 수 없는 숫자만큼의 argument를 받고 싶을 때 쓰는 파라미터이다.
, 위치 인자 (positional argument): 키워드 인자가 아닌 인자. 위치 인자들은 인자 목록의 처음에 나오거나 이터러블 의 앞에 *를 붙여 전달할 수 있다.
(출처 - https://www.geeksforgeeks.org/args-kwargs-python/ [GeeksforGeeks], https://armin.tistory.com/270 )


<키워드 인자> (Keyword arguments)
Keyword Argument 또한 함수를 호출할 때 값을 넘겨주는 방법으로 (위치에 상관없이) 직접 변수 이름을 사용한다. 알 수 없는 숫자만큼의 argument를 받고 싶을 때 쓰는 파라미터 이다.
(출처 - https://www.geeksforgeeks.org/args-kwargs-python/ [GeeksforGeeks])


<함수인자> (Arguments)
함수 인자(Arguments)는 함수로 전달되는 정보입니다. 예를 들어, 요리할때  레시피에서 사용(첨가)되는 재료처럼 생각할 수 있다. 참고로 "매개변수"는 함수를 정의할 때 사용되는 변수의 이름이다. 반면, "함수 인자"는 함수를 호출할 때 실제로 전달되는 값이다.
(출처 - args 와 kwargs 정리 )


<가변 인자> (args, variable argument)
args는 positional arguments를 뜻하며 여러 인자를 튜플로 묶고 이를 args에 할당한다.
가변 인자(variable argument)는 함수에 임의의 개수의 인자를 전달할 수 있게 해준다. 가변 인자는 def 함수명(*args):으로 표시되고 * 기호는 여러 개의 인자를 받을 수 있음을 나타내는 것이다.
(출처 - args 와 kwargs 정리 )


<키워드 인자 집합> (kwargs, variable argument)
kwargs는 keyword arguments를 뜻하며 여러 키워드 인자를 받고 딕셔너리로 만든다. 함수에 키워드 인자를 사전 형태로 전달할 수 있게 해주는 메커니즘으로 def 함수명(**kwargs):으로 표시된다. 그리고 이때 ** 기호는 키워드 인자들을 사전 형태로 받을 수 있음을 나타내는 것이다. 
(출처 - args 와 kwargs 정리 )
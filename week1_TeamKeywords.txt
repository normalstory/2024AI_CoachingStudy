<데이터 리터러시> (Data literacy)
데이터 리터러시는 자료를 정보로 읽고, 이해하고, 생성하고, 전달하는 능력입니다. (출처: 위키백과)


데이터를 이해하고 해석하며, 이를 바탕으로 의미 있는 결정을 내릴 수 있는 능력을 의미합니다. 이는 현대 사회에서 점점 더 중요해지고 있는 기술로, 다양한 분야에서 데이터를 활용하여 문제를 해결하고 가치를 창출하는 데 필수적입니다.


데이터 리터러시는 다음과 같은 여러 측면을 포함합니다:
- 데이터 이해: 데이터를 읽고, 데이터의 출처와 구조를 이해하며, 데이터가 무엇을 나타내는지 파악하는 능력입니다. 이는 숫자, 그래프, 표 등의 형태로 제공되는 데이터를 해석할 수 있어야 함을 의미합니다.
- 데이터 분석: 데이터를 분석하여 의미 있는 정보를 도출하는 능력입니다. 여기에는 기본적인 통계 분석, 데이터 시각화, 데이터 마이닝 등이 포함됩니다.
- 데이터 해석: 분석된 데이터를 해석하여 그 의미를 이해하고, 이를 바탕으로 결론을 도출하는 능력입니다. 이는 데이터가 보여주는 트렌드나 패턴을 파악하고, 이를 통해 문제를 해결하거나 새로운 인사이트를 얻는 과정을 포함합니다.
- 데이터 활용: 데이터를 실제 상황에 적용하여 의사 결정을 내리는 능력입니다. 이는 데이터 기반 의사 결정(Data-Driven Decision Making, DDDM)을 지원하며, 데이터에 근거한 전략 수립, 문제 해결, 성과 평가 등을 포함합니다.
- 데이터 윤리: 데이터를 사용할 때 지켜야 할 윤리적 기준과 법적 요구사항을 이해하고 준수하는 능력입니다. 이는 개인정보 보호, 데이터 보안, 데이터의 정확성과 신뢰성 등을 포함합니다.


데이터 리터러시가 중요한 이유는 다음과 같습니다:
- 의사 결정: 데이터에 기반한 의사 결정을 통해 더 정확하고 효율적인 결정을 내릴 수 있습니다.
- 문제 해결: 데이터를 분석하여 문제의 원인을 파악하고, 효과적인 해결책을 도출할 수 있습니다.
- 혁신 촉진: 데이터에서 새로운 인사이트를 얻어 새로운 기회를 발견하고, 혁신을 촉진할 수 있습니다.
- 경쟁력 강화: 데이터를 효과적으로 활용하여 경쟁 우위를 확보하고, 시장에서의 경쟁력을 강화할 수 있습니다.


따라서 데이터 리터러시는 개인뿐만 아니라 조직 전체가 데이터 중심의 사고방식을 갖추고, 데이터를 통해 더 나은 결과를 도출할 수 있도록 돕는 중요한 역량입니다.


<데이터 마이닝> (Data Mining)
대규모로 저장된 데이터 안에서 체계적이고 자동적으로 통계적 규칙이나 패턴을 분석하여 가치있는 정보를 추출하는 과정이다. (출처 - 위키피디아)


<데이터 전처리> (Data preprocessing)
데이터 전처리는 데이터를 분석하기 전에 데이터를 조작, 필터링 또는 확대하는 것을 의미할 수 있으며, 데이터 마이닝 프로세스에서 중요한 단계인 경우가 많다. 관련성이 없고 중복된 정보가 존재하거나 시끄럽고 신뢰할 수 없는 데이터의 비율이 높으면 훈련 단계에서 지식 발견이 더 어려울 수 있다. 데이터 준비 및 필터링 단계에는 상당한 처리 기간이 걸릴 수 있다. 데이터 전처리에 사용되는 방법의 예로는 정제, 인스턴스 선택, 정규화, 원-핫 인코딩, 데이터 변환, 특징 추출 및 특징 선택이 있다. (출처: 위키백과)


<데이터사이언스> (Data Science)
데이터 마이닝과 유사하게 정형, 비정형 형태를 포함한 다양한 데이터로부터 지식과 인사이트를 추출하는데 과학적 방법론, 프로세스, 알고리즘, 시스템을 동원하는 융합분야다.


데이터 사이언스의 주요 구성 요소:
- 데이터 수집(Data Collection):다양한 출처로부터 데이터를 수집합니다. 여기에는 데이터베이스, 웹 크롤링, 센서 데이터, API 등이 포함될 수 있습니다.
- 데이터 전처리(Data Preprocessing):수집된 데이터를 분석 가능한 형태로 변환합니다. 이 과정에는 데이터 정제, 결측값 처리, 데이터 변환, 이상치 제거 등이 포함됩니다.
- 데이터 분석(Data Analysis):데이터를 탐색하고, 통계적 기법을 사용하여 패턴을 찾고, 데이터를 요약합니다. 기술통계(descriptive statistics)와 탐색적 데이터 분석(EDA)이 이 과정에 포함됩니다.
- 데이터 모델링(Data Modeling):머신러닝 알고리즘과 통계 모델을 사용하여 데이터를 학습시키고, 예측 모델을 만듭니다. 회귀 분석, 분류, 군집화, 차원 축소 등의 기법이 사용됩니다.
- 데이터 시각화(Data Visualization):데이터를 시각적으로 표현하여 이해하기 쉽게 만드는 과정입니다. 그래프, 차트, 대시보드를 사용하여 데이터를 시각적으로 전달합니다.
- 결과 해석(Interpretation of Results):분석 결과를 해석하고, 이를 바탕으로 의미 있는 결론을 도출합니다. 결과가 비즈니스나 연구 질문에 어떻게 적용될 수 있는지 설명합니다.
- 의사 결정 지원(Decision Support):데이터 분석 결과를 바탕으로 의사 결정을 지원합니다. 이는 비즈니스 전략 수립, 정책 결정, 제품 개발 등 다양한 분야에서 활용됩니다.


데이터 사이언스의 주요 기법과 도구:
- 기술 통계(Descriptive Statistics):데이터의 기본적인 특성을 요약하고 설명하는 통계 기법입니다.
- 예측 분석(Predictive Analytics):과거 데이터를 기반으로 미래를 예측하는 분석 기법입니다. 머신러닝과 회귀 분석이 주요 도구로 사용됩니다.
- 데이터 마이닝(Data Mining):대규모 데이터에서 패턴이나 유의미한 정보를 추출하는 과정입니다.
- 머신러닝(Machine Learning):데이터를 기반으로 학습하고 예측하는 알고리즘을 개발하고 적용합니다.
- 빅 데이터 기술(Big Data Technologies):대규모 데이터를 처리하고 분석하기 위해 Hadoop, Spark 같은 빅 데이터 프레임워크를 사용합니다.
- 프로그래밍 언어:Python, R, SQL 등 데이터 분석과 처리를 위한 프로그래밍 언어입니다.


데이터 사이언스의 응용 분야:
- 비즈니스: 마케팅 전략, 고객 세분화, 수요 예측, 재무 분석 등.
- 의료: 질병 진단, 환자 관리, 의료 이미지 분석 등.
- 공공정책: 범죄 예측, 교통 관리, 환경 모니터링 등.
- 스포츠: 경기 분석, 선수 퍼포먼스 최적화 등.
- 기술: 추천 시스템, 자연어 처리, 컴퓨터 비전 등.


데이터 사이언티스트의 역할: 데이터 사이언티스트는 다양한 데이터를 분석하여 조직의 문제를 해결하고, 데이터 기반의 의사 결정을 지원하는 역할을 합니다. 데이터 사이언티스트는 다음과 같은 업무를 수행합니다:
- 데이터 수집 및 전처리
- 데이터 분석 및 시각화
- 머신러닝 모델 개발 및 평가
- 분석 결과 해석 및 보고서 작성
- 비즈니스 문제 해결을 위한 데이터 기반 전략 제안


데이터 사이언스는 데이터의 가치와 활용 가능성을 최대한 끌어내어 다양한 분야에서 혁신을 이끌어내는 중요한 분야입니다.


<도메인 경험> (Domain Experience)
도메인 경험이란 특정 산업이나 분야에 대한 깊이 있는 지식과 이해를 말합니다. 도메인 경험은 데이터를 분석하고 해석하는 과정에서 매우 중요한 역할을 합니다. 이를 통해 데이터 과학자는 단순히 데이터의 패턴을 발견하는 것 이상으로, 이러한 패턴이 해당 분야에서 어떤 의미를 가지는지 이해할 수 있습니다.


도메인 경험이 중요한 이유는 다음과 같습니다:
- 문제 정의: 데이터 과학 프로젝트는 명확한 문제 정의에서 시작됩니다. 도메인 경험이 있으면, 중요한 문제를 정확하게 식별하고 적절한 질문을 제기할 수 있습니다.
- 데이터 이해: 도메인 경험을 통해 데이터가 어떻게 생성되고 수집되었는지, 그리고 데이터의 의미가 무엇인지 이해할 수 있습니다. 이는 데이터 전처리 과정에서 매우 유용합니다.
- 모델링: 도메인 지식을 활용하여 적절한 모델을 선택하고, 모델의 결과를 해석하며, 비즈니스 맥락에서 의미 있는 결론을 도출할 수 있습니다.
- 결과 해석: 데이터 분석의 결과를 도메인 내에서 어떻게 적용할지 결정하는 데 있어 도메인 경험은 필수적입니다. 이는 분석 결과가 실제 비즈니스에 어떻게 영향을 미칠지 예측하는 데 도움이 됩니다.
- 의사 결정 지원: 데이터 과학의 최종 목표는 데이터를 기반으로 더 나은 의사 결정을 내리는 것입니다. 도메인 경험은 데이터 과학자가 비즈니스 전략에 대한 통찰력을 제공하고, 의사 결정자들에게 유용한 조언을 할 수 있도록 돕습니다.


예를 들어, 의료 분야의 데이터 사이언티스트는 의료 데이터, 환자 기록, 질병의 진단 및 치료 방법 등에 대한 깊은 이해가 필요합니다. 금융 분야에서는 금융 상품, 시장 동향, 리스크 관리 등에 대한 지식이 중요합니다.
결론적으로, 데이터 사이언스에서 도메인 경험은 데이터를 효과적으로 분석하고, 그 결과를 실제 비즈니스나 산업에 적용하는 데 있어 매우 중요한 역할을 합니다.


<딥러닝> (Deep Learning)
딥러닝(Deep Learning)은 머신러닝의 한 분야로, 인공신경망(Artificial Neural Networks)을 기반으로 하여 데이터를 학습하고 복잡한 패턴을 인식하는 기술입니다. 딥러닝은 특히 대량의 데이터를 처리하고 분석하는 데 탁월하며, 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 뛰어난 성능을 발휘하고 있습니다.


딥러닝의 주요 개념과 구성 요소는 다음과 같습니다:
- 인공신경망(Artificial Neural Networks): 생물학적 신경망에서 영감을 받아 개발된 구조로, 입력 데이터를 처리하고 학습하는 알고리즘입니다. 신경망은 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성됩니다.
- 뉴런(Neuron): 신경망의 기본 단위로, 입력을 받아 가중치와 함께 처리하고 활성화 함수를 통해 출력을 생성합니다.
- 가중치(Weight): 입력 데이터의 중요도를 조정하는 파라미터로, 학습 과정에서 조정됩니다.
- 활성화 함수(Activation Function): 뉴런의 출력을 비선형 변환하여 신경망의 표현력을 높이는 함수입니다. 예: ReLU(Rectified Linear Unit), 시그모이드(Sigmoid), 탄젠트 하이퍼볼릭(Tanh).
- 심층신경망(Deep Neural Networks, DNN): 여러 개의 은닉층을 가진 인공신경망으로, 복잡한 데이터를 학습하고 고차원적인 패턴을 인식할 수 있습니다. 은닉층이 많을수록 "깊은" 신경망이라고 합니다.
- 합성곱 신경망(Convolutional Neural Networks, CNN): 주로 이미지 인식에 사용되는 신경망으로, 합성곱 층(Convolutional Layers)과 풀링 층(Pooling Layers)을 통해 이미지의 공간적 특징을 추출합니다.
- 합성곱 층: 필터를 사용하여 이미지의 특징을 추출합니다.
- 풀링 층: 특징 맵의 크기를 줄여 계산량을 줄이고, 중요한 특징을 강조합니다.
- 순환 신경망(Recurrent Neural Networks, RNN): 시계열 데이터나 순차 데이터를 처리하는 데 사용되는 신경망으로, 이전 입력의 정보를 기억하여 현재 입력과 함께 처리합니다. LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)와 같은 변형이 있습니다.
- 오토인코더(Autoencoder): 입력 데이터를 압축(인코딩)하고, 다시 복원(디코딩)하는 방식으로 특징을 학습하는 신경망입니다. 주로 차원 축소와 노이즈 제거에 사용됩니다.
- 생성적 적대 신경망(Generative Adversarial Networks, GAN): 두 개의 신경망(생성자와 판별자)이 경쟁하며 학습하는 구조로, 현실적인 데이터를 생성하는 데 사용됩니다. 예: 이미지 생성, 스타일 변환.


딥러닝의 주요 특징:
- 대용량 데이터 처리: 대량의 데이터를 효과적으로 학습하고, 고차원적 패턴을 인식할 수 있습니다.
- 자기 학습: 복잡한 특징을 자동으로 추출하고 학습하여, 수동으로 특징을 설계할 필요가 없습니다.
- 다양한 응용 분야: 이미지 인식, 음성 인식, 자연어 처리, 자율 주행, 추천 시스템 등 다양한 분야에서 활용됩니다.


딥러닝의 성장은 컴퓨팅 파워의 발전과 대량의 데이터 접근성 증가 덕분에 가능해졌습니다. 이는 딥러닝이 복잡하고 고차원적인 문제를 해결하는 데 매우 유용한 도구로 자리잡게 했습니다.


<머신러닝> (Machine Learning)
머신러닝(Machine Learning)은 인공지능(AI)의 한 분야로, 컴퓨터가 명시적으로 프로그래밍되지 않아도 데이터를 통해 학습하고, 그 경험을 바탕으로 예측이나 결정을 내릴 수 있도록 하는 기술입니다. 머신러닝은 다양한 알고리즘과 통계 모델을 사용하여 데이터를 분석하고, 패턴을 인식하며, 학습한 내용을 토대로 새로운 데이터를 처리합니다.


머신러닝의 주요 구성 요소와 개념은 다음과 같습니다:
- 데이터셋: 머신러닝 모델을 훈련시키기 위해 사용하는 데이터 모음입니다. 데이터셋은 입력 데이터(features)와 목표 변수(target)로 구성됩니다.
- 알고리즘: 머신러닝 모델을 구축하기 위해 사용하는 방법론이나 절차입니다. 알고리즘은 데이터를 분석하고, 패턴을 찾고, 예측을 수행하는 데 사용됩니다.
- 훈련(Training): 머신러닝 모델이 데이터를 학습하는 과정입니다. 모델은 훈련 데이터셋을 사용하여 패턴을 인식하고, 이를 기반으로 예측할 수 있도록 조정됩니다.
- 검증(Validation): 모델이 훈련된 후, 모델의 성능을 평가하기 위해 별도의 검증 데이터셋을 사용합니다. 이를 통해 모델의 과적합(overfitting) 여부를 판단하고, 모델을 튜닝합니다.
- 테스트(Test): 최종 모델의 성능을 평가하기 위해 사용되는 과정입니다. 테스트 데이터셋을 통해 모델이 실제 환경에서 얼마나 잘 작동하는지 확인합니다.
- 모델(Model): 데이터와 알고리즘을 기반으로 구축된 예측 모델입니다. 이 모델은 새로운 데이터에 대해 예측을 수행합니다.


머신러닝의 주요 유형은 다음과 같습니다:
- 지도학습(Supervised Learning): 입력 데이터와 이에 대응하는 정답(레이블)을 사용하여 모델을 학습시킵니다. 예를 들어, 주택 가격 예측, 스팸 이메일 필터링 등이 포함됩니다.
- 분류(Classification): 입력 데이터를 다양한 범주로 분류하는 문제입니다. 예: 이메일 스팸 필터링(스팸/스팸 아님).
- 회귀(Regression): 연속적인 값을 예측하는 문제입니다. 예: 주택 가격 예측.
- 비지도학습(Unsupervised Learning): 정답이 없는 데이터를 사용하여 모델을 학습시킵니다. 데이터의 구조나 패턴을 찾는 데 사용됩니다. 예: 군집화(Clustering), 차원 축소(Dimensionality Reduction).
- 군집화(Clustering): 데이터 포인트들을 비슷한 특성을 가진 그룹으로 묶는 문제입니다. 예: 고객 세분화.
- 연관 규칙 학습(Association Rule Learning): 데이터 내에서 항목 간의 연관성을 찾는 문제입니다. 예: 장바구니 분석.
- 강화학습(Reinforcement Learning): 에이전트가 환경과 상호작용하며 보상을 최대화하는 방향으로 행동을 학습합니다. 예: 게임 플레이, 로봇 제어.


머신러닝은 다양한 산업 분야에서 널리 사용되고 있으며, 예측 분석, 자연어 처리, 이미지 인식, 자율 주행 차량, 추천 시스템 등 많은 응용 분야에서 중요한 역할을 하고 있습니다.


<보외법> (extrapolation) 
이용가능한 자료의 범위가 한정되어 있어 그 범위 이상의 값을 구할 수 없을 때, 관측된 값을 이용해서 한계점 이상의 값을 추정하는 것을 의미한다. 그림처럼 주어진 A~B의 범위 내 P값을 이용하여 Q값을 예측하는 것이며, 예측의 정확도는 떨어지게 된다.
  
출처 : 두산백과


<수학 함수> (Mathematical functions) 
분산(variance)은 관측값에서 평균을 뺀 값을 제곱하고, 그것을 모두 더한 후 전체 수로 나눠서 구한다. 즉, 차이값의 제곱의 평균이다. 관측값에서 평균을 뺀 값인 편차를 모두 더하면 0이 나오므로 편차의 부호를 없애기 위해 제곱해서 더한다. 또한 분산은 표준편차의 제곱이다.


std(Standard deviation 표준편차) : 자료의 관찰값들이 얼마나 흩어져 있는지 그 정도를 하나의 수치로 나타내는 방법 중 하나이다. 표준편차(standard deviation)는 분산을 제곱근한 것이다. 통계집단의 분산의 정도 또는 자료의 산포도를 나타내는 수치로, 분산의 음이 아닌 제곱근 한 것으로 정의된다. 표준편차가 작을수록 평균값에서 변량들의 거리가 가깝다. 통계학과 확률에서 주로 확률의 분포, 확률변수 혹은 측정된 인구나 중복집합에 적용된다.( 출처 : 위키백과)
쉽게말해 제곱해서 값이 뻥튀기 된 분산을 제곱근해서 다시 원래 크기로 만들어준다.특히, 서술되는 방식에 따라 그 함의가 다양해서 이후 연구방법론의 핵심인 추론통계의 기초가 되는 모 표준 편차, 표본 표준 편차, 표준오차를 이해하는 데 의외(?)로 중요한 역할을 하게 된다. 이처럼 표준편차가 중요한 역할을 하는 데는 사실 표준편차는 어떤 수의 크고 작음을 직관적으로 이야기해 줄 수 있기 때문이다.( 출처 :  나무위키)
  

- exponential function(지수 함수):
거듭제곱의 지수를 변수로 하고, 정의역을 실수 전체로 정의하는 초월함수이다. 로그 함수의 역함수이다.( 출처 : 위키백과)
참고로, numpy 내 지수 함수는 exp, expml, exp2, log, 10g10, loglp, 10g2, power, sqrt 들이 있다.( 출처 : 개발자로 취업하기)
  

- trigonometric functions(삼각 함수) :
수학에서 삼각함수는 각의 크기를 삼각비로 나타내는 함수이다. 즉, 삼각형의 각도와 변의 길이의 관계를 나타낸 것이다. 예각 삼각함수는 직각 삼각형의 예각에 직각 삼각형의 두 변의 길이의 비를 대응시킨다. 임의의 각의 삼각함수 역시 정의할 수 있다( 출처 :  위키백과)
참고로, numpy 내 삼각 함수는 sin, cos, tan, acsin, arccos, atctan 들이 있다.( 출처 : 개발자로 취업하기)


- hyperbolic functions(쌍곡선 함수) :  
수학에서 쌍곡선 함수는 일반적인 삼각함수와 유사한 성질을 갖는 함수로 삼각함수가 단위원 그래프를 매개변수로 표시할 때 나오는 것처럼, 표준쌍곡선을 매개변수로 표시할 때 나온다.( 출처 : 위키백과)
참고로, numpy 내 쌍곡선 함수는 sinh, cosh, tanh, acsinh, arccosh, atctanh 들이 있다.( 출처 : 개발자로 취업하기)


<오버 피팅> (Overfitting)
train-set에 너무 과하게 모델이 최적화된 상태 (출처 - [인공지능 기초] 3. Over-Fitting(과적합))
  
(좌 - Overfitted, 우 - Good Fit)


<인공신경망> (ANNs, Artificial Neural Networks)
인공신경망은 기계 학습(machine learning)과 인지과학에서 생물학의 신경망(동물의 중추신경계중 특히 뇌)에서 영감을 얻은 알고리즘이다. 인공신경망은 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 가리킨다. 인공신경망에는 교사 신호(정답)의 입력에 의해서 문제에 최적화되어 가는 지도 학습과 교사 신호를 필요로 하지 않는 비지도 학습으로 나뉘어 있다. (출처: 위키백과)


<자료 구조 유형> (Data structure type)
비직선적 구조를 가지는 데이터 (Non-linear Structure)
: 데이터의 구조가 비직선적으로 주어질 경우, 이러한 특징이 있는 데이터를 말합니다. 일반적으로는 이차원 공간에서 직선이나 곡선을 통해 표현할 수 있습니다. t-SNE나 autoencoder와 같은 알고리즘이 많이 활용됩니다. 
- Image data: 이미지 데이터는 비직선적 구조를 가지는 대표적인 예입니다. 이미지의 각 부분은 서로 다른 방향으로 연결되어 있지만, 특정한 구조를 형성하고 있습니다.
- Text data: 텍스트 데이터도 비직선적 구조를 가질 수 있습니다. 문장이나 문서의 각 단락은 서로 다른 방향으로 연결되어 있고, 특정한 의미나 맥락을 가지고 있습니다.


직선적 구조를 가지는 데이터 (Linear Structure)
: 데이터의 구조가 직선적으로 주어질 경우, 이러한 특징이 있는 데이터를 말합니다. 일반적으로는 이차원 공간에서 직선을 통해 표현할 수 있습니다. PCA나 LLE와 같은 알고리즘이 자주 활용됩니다. 
- Time series data: 시간 시리즈 데이터는 직선적 구조를 가지는 대표적인 예입니다. 각 단락은 이전 단락과 이어져 있고, 일정한 패턴을 형성하고 있습니다.
- Stock price data: 주식 가격 데이터도 직선적 구조를 가질 수 있습니다. 주식의 가격이 시간에 따라 일직으로 증가하거나 감소하고 있습니다.


<차원 축소> (Dimensionality Reduction)
고차원 데이터로부터, 저차원의 데이터로 변환하는 방법 (출처 - 위키백과)
매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성 (출처 - 차원 축소 (Dimension Reduction) - PCA, LDA)


<차원의 저주> (The curse of dimensionality)
차원의 저주는 데이터를 분석할 때 차원이 너무 많아지면 생기는 문제로 차원이 많아질수록 데이터가 넓게 퍼지고, 분석이 어려워지는 현상을 말한다.
데이터의 차원이 높아질 수록 알고리즘의 실행이 아주 까다로워지는 일이다. 차원의 저주 현상은 수치 분석, 샘플링, 조합, 기계 학습, 데이터 마이닝 및 데이터베이스와 같은 영역에서 발생한다. 이러한 문제의 공통 주제는 차원이 증가하면 공간의 부피가 너무 빨리 증가하여 사용 가능한 데이터가 희소해진다는 것이다. 신뢰할 수 있는 결과를 얻기 위해 필요한 데이터의 양이 차원에 따라 기하급수적으로 증가하는 경우가 많다. 또한 데이터 구성 및 검색은 종종 개체가 유사한 속성을 가진 그룹을 형성하는 영역을 감지하는 데 의존한다. 그러나 고차원 데이터에서는 모든 객체가 여러 면에서 희소하고 유사하지 않아 공통 데이터 구성 전략이 효율적이지 못하다. (출처 : 위키백과)
일상 경험의 3차원 물리적 공간과 같은 저차원 환경에서는 발생하지 않는 고차원 공간에서 데이터를 분석하고 정리할 때 발생하는 다양한 현상 (출처 - 위키백과)
차원이 증가하면서 학습데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상을 일컫는다. 차원이 증가할수록 변수가 증가하고, 개별 차원 내에서 학습할 데이터 수가 적어진다. (출처 - 차원의 저주 개념, 발생 원인과 해결 방법)


학습 데이터 수와 차원의 수의 관계
- 데이터 희소성 증가:차원이 많아지면, 데이터 포인트들이 공간에서 더 넓게 퍼지게 돼요. 마치 큰 운동장에 몇 명만 있는 것처럼요.
- 학습 데이터 수의 필요성 증가:차원이 많아질수록 더 많은 데이터가 필요해요. 예를 들어, 각 차원에 대해 충분한 샘플을 갖추기 위해서 아주 많은 데이터가 필요하게 돼요.


차원의 저주 현상의 원인
- 거리 측정의 왜곡:고차원 공간에서는 두 점 간의 거리가 대부분 비슷해져서, 데이터 간의 유사성을 판단하기 어려워져요.
- 공간의 부피 증가:차원이 증가할수록 공간의 부피가 기하급수적으로 커져서, 데이터가 차지하는 부분이 매우 작아져요.
- 모델 복잡성 증가:차원이 많아지면, 모델이 복잡해져서 과적합(overfitting)이라는 문제가 생길 수 있어요. 과적합은 모델이 학습 데이터에 너무 맞춰져서 새로운 데이터를 잘 예측하지 못하는 현상이에요.


차원의 저주 해결방법
- 특징 선택(Feature Selection):필요 없는 특징을 제거하고 중요한 특징만을 선택해서 차원을 줄이는 방법이에요. 예를 들어, 사람의 키와 몸무게만 선택해서 분석하는 거예요.
- 특징 추출(Feature Extraction):기존의 고차원 특징들을 조합해서 새로운 저차원 특징을 만드는 방법이에요. 예를 들어, 여러 과목 성적을 종합해서 '성적 점수' 하나로 만드는 것과 비슷해요.


해결방법의 종류와 특징
- PCA (Principal Component Analysis)
1. 특징:PCA는 데이터를 선형적으로 줄여요. 즉, 데이터를 일직선처럼 간단하게 만드는 거예요.
2. 장점:계산이 빠르고, 차원을 줄이면서도 데이터의 분산을 최대한 유지해요.
3. 단점:데이터의 선형 관계만을 고려해서, 비선형 구조를 잘 반영하지 못해요.
- t-SNE (t-Distributed Stochastic Neighbor Embedding)
1. 특징:t-SNE는 데이터를 비선형적으로 줄여요. 복잡한 데이터의 모양을 잘 보존하면서 줄여줘요.
2. 장점:데이터의 복잡한 비선형 구조를 잘 반영해서, 저차원 공간에서 군집(Clusters)을 잘 나타내요.
3. 단점:계산 비용이 높고, 대규모 데이터에 적용하기 어려워요.


<튜플> (Tuple)
리스트는 프로그램 실행 중에 값이 바뀔 수 있습니다. 하지만 프로그램 실행 도중 값이 변경되지 않도록 하는 리스트가 필요할 때도 있습니다. 그럴 때 사용하는 것이 튜플입니다.
튜플은 대괄호[ ] 대신 --소괄호( )를 사용(단, 괄호가 필수는 아니고 콤마가 필수)--한다는 점을 제외하면 리스트와 동일합니다.
예)  t1= (300, 500) t1= 300, 500
하나의 원소만 존재하는 경우는 tuple(튜플)이 되지 않습니다. 하지만 한 개의 원소 뒤에 콤마를 찍어주면 tuple이 유지할 수 있습니다. 튜플을 정의하는 것은 소괄호(())가 아닌 콤마(,)이기 때문입니다.
예) t2 = (20,)
출처: https://wikidocs.net/16042


<특징> (Features)
특징은 기계 학습(machine learning)과 패턴 인식(pattern recognition)의 용어로, 관찰 대상에게서 발견된 개별적이고 측정가능한 경험적 속성을 말한다. (출처: 위키백과)
머신러닝이나 데이터 분석에 사용되는 개별 독립 변수를 의미 (출처 - 인공지능(AI) & 머신러닝(ML) 사전)


<특징 벡터> (Feature Vector)
특징들의 집합을 특징 벡터(feature vector)라고 한다. 굳이 벡터로 표시하는 이유는 수학적으로 다루기 편하기 때문이다. 피쳐라는 개념을 가져온 것은 선형 회귀와 같은 통계학적인 기법에서이다. 독립 변수와 종속 변수 개념 또한 통계학에서 가져왔다. (출처: 위키백과)


<프로그램 언어 Typing 유형> (Typing in the programming language)
프로그래밍 언어들은 각자 기본적인 자료형인 Primitive Data Type을 가진다. 이러한 자료형들을 어떻게 결정하는지를 Typing이라고 한다. 즉, 처음 특정 데이터의 data type을 결정하는 것을 Typing이라고 한다. Python은 기본적으로 Dynamic typing이 지원된다. 일반적으로 Typing은 Static Typing과 Dynamic Typing으로 구분된다. 
예) a_ndarray = np.array(a, --int--)

1. Dynamic typing 동적 타이핑
- a = 15
- 코드를 작성하는데 있어서 컴퓨터적 구조를 생략한다.
- 코드를 작성하는 시간이 빠르다.
- 코드를 실행하는 속도가 느리다.
- 코드의 내용, 로직을 파악하기 쉽다.
- 처음 프로그래밍을 학습하는 사람에게 적합한 언어이다.
- 속도를 중요시하는 작업에선 사용하기 부적합하지만 작고 단순한 프로젝트를 하기엔 적합하다.
- 동적 타이핑을 사용하는 언어 - Groovy, JavaScript, Lisp, Lua, Objective-C, PHP, Prolog, Python, Ruby, Smalltalk, Tcl
- 동적타입 언어는 런타임 시 확인할 수 밖에 없기 때문에, 이러한 불편함을 해소하기 위해 TypeScipt와 Flow같은 툴이 나왔다

2. Static typing 정적 타이핑
- int a = 15
- 동적타이핑과 정반대로 코드를 작성할 때 컴퓨터적 구조를 명시한다
- 코드를 작성하는 시간이 느리다.
- 코드를 실행하는 속도가 빠르다.
- 코드의 구조를 파악하기 쉽다.
- 처음 프로그래밍 언어를 학습하는 사람들에겐 어려울 수 있다.
- 크고 복잡하며 여러 사람들이 함께 참여하는 프로젝트에 적합하다.
- 정적 타이핑을 사용하는 언어 - Ada, C, C++, C#, JADE, Java, Fortran, Haskell, ML, Pascal, Scala
- 컴파일 시 에러가 뜨기 때문에, 더 편하고 빠르게 에러 확인이 가능하다.
출처 : Python Dynamic Typing ,https://seongonion.tistory.com/16#정리-1


<하이퍼 파라미터> (Hyper Parameters)
하이퍼 파라미터는 데이터 과학자가 기계 학습 모델 훈련을 관리하는 데 사용하는 외부 구성 변수입니다. 때때로 모델 하이퍼 파라미터라고 부르며, 하이퍼 파라미터는 모델을 훈련하기 전에 수동으로 설정됩니다. 하이퍼 파라미터는 파라미터와는 다르며, 데이터 과학자에 의해 설정되는 것이 아닌 학습 프로세스 중에 자동으로 파생되는 내부 파라미터입니다. 하이퍼 파라미터의 예로는 신경망의 노드 및 계층 수와 의사 결정 트리의 분기 수가 있습니다. 하이퍼 파라미터는 모델 아키텍처, 학습 속도 및 모델 복잡성과 같은 주요 기능을 결정합니다. (출처 : Amazon)


<dtype> (Data type objects) 
NumPy dtype 객체: 파이썬에서 과학적 계산을 위한 데이터 유형이다 
보유 속성
- kind: 데이터 유형의 종류 ('i' for integer, 'f' for float, etc.)
- itemsize: 각 항목의 바이트 크기
- shape: 항목의 형상
- byteswap: 바이트 순서 (big-endian 또는 little-endian)
사용법
- NumPy 배열 생성: np.array(data, dtype)
- 데이터 유형 변환: array.astype(dtype)
- dtype 객체 비교: dtype1 == dtype2
사용 예
- 정수 배열 생성 : arr1 = np.array([1, 2, 3], dtype=np.int32)
- 부동 소수점 배열 생성 : arr2 = np.array([1.1, 2.2, 3.3], dtype=np.float64)
- 문자열 배열 생성 : arr3 = np.array(["a", "b", "c"], dtype=np.string_)
- 데이터 유형 변환 : arr1 = arr1.astype(np.float32)
- dtype 객체 비교 : np.dtype(np.int32) == np.dtype("int32")
주의 사항
- dtype 객체는 NumPy 배열의 필수 요소입니다.
- 데이터 유형을 변환하면 데이터 손실이 발생할 수 있습니다.
- 구조화된 데이터 유형을 사용할 때는 주의가 필요합니다.
- 잘못된 데이터 유형을 지정하면 TypeError가 발생합니다.
- 데이터 유형 변환이 불가능하면 ValueError가 발생합니다.
출처 : https://runebook.dev/ko/docs/numpy/reference/arrays.dtypes


<LLM> (Large Language Model)
방대한 양의 텍스트 데이터를 학습하여 다양한 종류의 텍스트를 생성할 수 있는 능력을 갖춘 모델입니다. 예를 들어, 시, 코드, 대본, 음악 작품, 이메일, 편지 등과 같은 다양한 종류의 텍스트를 생성할 수 있습니다. 또한, 텍스트를 번역하거나, 요약하거나, 질문에 답하는 등의 작업에도 사용할 수 있습니다. (출처: datamaker)


<LMM> (Large Multimodal Model)
텍스트 데이터 외에도 이미지, 오디오 등 여러 가지 유형의 데이터를 통합하여 처리할 수 있는 능력을 갖춘 모델입니다. 예를 들어, 영화, 음악, 뉴스 등 다양한 미디어 콘텐츠 요약 및 생성, 음성 인식, 이미지 인식, 감정 분석 등 여러가지 유형의 데이터를 처리하는 작업에 사용될 수 있습니다. (출처: datamaker)


<K-NN 최근접 이론 알고리즘> (K-Nearest Neighbor)
가장 가까운 훈련 데이터 포인트 k개를 최근접 이웃으로 찾아 예측에 사용하는 알고리즘이다. k 가 1이라면 가장 가까운 포인트 1개를 예측에 사용하고, k가 3이면 3개를 예측에 사용한다. k가 너무 작으면 아주 근처의 점 하나에 민감하게 반응하기 때문에 과적합(overfitting)의 발생 가능성이 높아진다.
